(myenv) user@ubuntu:~/Coding/WebDev/Face-Emotion-Detection/Model-Training$ node index.js 
2025-04-19 10:07:22.024549: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-19 10:07:22.044683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading data for usage: Training...
Loaded 28709 samples for usage: Training.
Loading data for usage: PublicTest...
Loaded 3589 samples for usage: PublicTest.

Creating model...
__________________________________________________________________________________________
Layer (type)                Input Shape               Output shape              Param #   
==========================================================================================
conv2d_Conv2D1 (Conv2D)     [[null,48,48,1]]          [null,46,46,64]           640       
__________________________________________________________________________________________
conv2d_Conv2D2 (Conv2D)     [[null,46,46,64]]         [null,46,46,64]           36928     
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,46,46,64]]         [null,46,46,64]           256       
__________________________________________________________________________________________
max_pooling2d_MaxPooling2D1 [[null,46,46,64]]         [null,23,23,64]           0         
__________________________________________________________________________________________
dropout_Dropout1 (Dropout)  [[null,23,23,64]]         [null,23,23,64]           0         
__________________________________________________________________________________________
conv2d_Conv2D3 (Conv2D)     [[null,23,23,64]]         [null,23,23,128]          73856     
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,23,23,128]]        [null,23,23,128]          512       
__________________________________________________________________________________________
conv2d_Conv2D4 (Conv2D)     [[null,23,23,128]]        [null,23,23,128]          147584    
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,23,23,128]]        [null,23,23,128]          512       
__________________________________________________________________________________________
max_pooling2d_MaxPooling2D2 [[null,23,23,128]]        [null,11,11,128]          0         
__________________________________________________________________________________________
dropout_Dropout2 (Dropout)  [[null,11,11,128]]        [null,11,11,128]          0         
__________________________________________________________________________________________
conv2d_Conv2D5 (Conv2D)     [[null,11,11,128]]        [null,11,11,256]          295168    
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,11,11,256]]        [null,11,11,256]          1024      
__________________________________________________________________________________________
conv2d_Conv2D6 (Conv2D)     [[null,11,11,256]]        [null,11,11,256]          590080    
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,11,11,256]]        [null,11,11,256]          1024      
__________________________________________________________________________________________
max_pooling2d_MaxPooling2D3 [[null,11,11,256]]        [null,5,5,256]            0         
__________________________________________________________________________________________
dropout_Dropout3 (Dropout)  [[null,5,5,256]]          [null,5,5,256]            0         
__________________________________________________________________________________________
conv2d_Conv2D7 (Conv2D)     [[null,5,5,256]]          [null,5,5,512]            1180160   
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,5,5,512]]          [null,5,5,512]            2048      
__________________________________________________________________________________________
conv2d_Conv2D8 (Conv2D)     [[null,5,5,512]]          [null,5,5,512]            2359808   
__________________________________________________________________________________________
batch_normalization_BatchNo [[null,5,5,512]]          [null,5,5,512]            2048      
__________________________________________________________________________________________
max_pooling2d_MaxPooling2D4 [[null,5,5,512]]          [null,2,2,512]            0         
__________________________________________________________________________________________
dropout_Dropout4 (Dropout)  [[null,2,2,512]]          [null,2,2,512]            0         
__________________________________________________________________________________________
flatten_Flatten1 (Flatten)  [[null,2,2,512]]          [null,2048]               0         
__________________________________________________________________________________________
dense_Dense1 (Dense)        [[null,2048]]             [null,512]                1049088   
__________________________________________________________________________________________
dropout_Dropout5 (Dropout)  [[null,512]]              [null,512]                0         
__________________________________________________________________________________________
dense_Dense2 (Dense)        [[null,512]]              [null,256]                131328    
__________________________________________________________________________________________
dropout_Dropout6 (Dropout)  [[null,256]]              [null,256]                0         
__________________________________________________________________________________________
dense_Dense3 (Dense)        [[null,256]]              [null,128]                32896     
__________________________________________________________________________________________
dropout_Dropout7 (Dropout)  [[null,128]]              [null,128]                0         
__________________________________________________________________________________________
dense_Dense4 (Dense)        [[null,128]]              [null,7]                  903       
==========================================================================================
Total params: 5905863
Trainable params: 5902151
Non-trainable params: 3712
__________________________________________________________________________________________

Training model...
Epoch 1 / 100
eta=0.0 ===========================================================================================> 
751402ms 29081us/step - acc=0.219 loss=1.93 val_acc=0.248 val_loss=1.81 
Epoch 2 / 100
eta=0.0 ===========================================================================================> 
923537ms 35743us/step - acc=0.246 loss=1.83 val_acc=0.249 val_loss=1.79 
Epoch 3 / 100
eta=0.0 ===========================================================================================> 
882588ms 34159us/step - acc=0.260 loss=1.80 val_acc=0.255 val_loss=1.79 
Epoch 4 / 100
eta=0.0 ===========================================================================================> 
912065ms 35299us/step - acc=0.318 loss=1.71 val_acc=0.343 val_loss=1.66 
Epoch 5 / 100
eta=0.0 ==============================================================================================> 
917235ms 35499us/step - acc=0.380 loss=1.58 val_acc=0.369 val_loss=1.55 
Epoch 6 / 100
eta=0.0 ===========================================================================================> 
887314ms 34341us/step - acc=0.416 loss=1.50 val_acc=0.323 val_loss=1.61 
Epoch 7 / 100
eta=0.0 ===========================================================================================> 
1018410ms 39415us/step - acc=0.437 loss=1.45 val_acc=0.442 val_loss=1.45 
Epoch 8 / 100
eta=0.0 ==============================================================================================> 
1133723ms 43878us/step - acc=0.462 loss=1.40 val_acc=0.458 val_loss=1.39 
Epoch 9 / 100
eta=0.0 ==============================================================================================> 
1177981ms 45591us/step - acc=0.483 loss=1.36 val_acc=0.480 val_loss=1.32 
Epoch 10 / 100
eta=0.0 ==============================================================================================> 
1148129ms 44436us/step - acc=0.503 loss=1.32 val_acc=0.500 val_loss=1.30 
Epoch 11 / 100
eta=0.0 ==============================================================================================> 
1135302ms 43939us/step - acc=0.513 loss=1.29 val_acc=0.555 val_loss=1.17 
Epoch 12 / 100
eta=0.0 ==============================================================================================> 
1105795ms 42797us/step - acc=0.526 loss=1.27 val_acc=0.563 val_loss=1.17 
Epoch 13 / 100
eta=0.0 ==============================================================================================> 
873941ms 33824us/step - acc=0.542 loss=1.24 val_acc=0.568 val_loss=1.23 
Epoch 14 / 100
eta=0.0 ==============================================================================================> 
893212ms 34570us/step - acc=0.548 loss=1.22 val_acc=0.529 val_loss=1.25 
Epoch 15 / 100
eta=0.0 ==============================================================================================> 
870948ms 33708us/step - acc=0.555 loss=1.19 val_acc=0.580 val_loss=1.13 
Epoch 16 / 100
eta=0.0 ==============================================================================================> 
875889ms 33899us/step - acc=0.572 loss=1.17 val_acc=0.540 val_loss=1.19 
Epoch 17 / 100
eta=0.0 ==============================================================================================> 
957150ms 37044us/step - acc=0.579 loss=1.15 val_acc=0.515 val_loss=1.23 
Epoch 18 / 100
eta=0.0 ==============================================================================================> 
1258596ms 48711us/step - acc=0.589 loss=1.12 val_acc=0.608 val_loss=1.06 
Epoch 19 / 100
eta=0.0 ==============================================================================================> 
1164629ms 45074us/step - acc=0.598 loss=1.10 val_acc=0.589 val_loss=1.10 
Epoch 20 / 100
eta=0.0 ==============================================================================================> 
1140456ms 44139us/step - acc=0.610 loss=1.08 val_acc=0.615 val_loss=1.04 
Epoch 21 / 100
eta=0.0 ==============================================================================================> 
1141661ms 44185us/step - acc=0.617 loss=1.05 val_acc=0.619 val_loss=1.04 
Epoch 22 / 100
eta=0.0 ==============================================================================================> 
1093037ms 42303us/step - acc=0.624 loss=1.04 val_acc=0.586 val_loss=1.12 
Epoch 23 / 100
eta=0.0 ==============================================================================================> 
889903ms 34442us/step - acc=0.633 loss=1.02 val_acc=0.628 val_loss=1.02 
Epoch 24 / 100
eta=0.0 ==============================================================================================> 
999345ms 38677us/step - acc=0.640 loss=0.992 val_acc=0.626 val_loss=1.01 
Epoch 25 / 100
eta=0.0 ==============================================================================================> 
919043ms 35569us/step - acc=0.644 loss=0.986 val_acc=0.614 val_loss=1.05 
Epoch 26 / 100
eta=0.0 ==============================================================================================> 
850379ms 32912us/step - acc=0.651 loss=0.966 val_acc=0.623 val_loss=1.01 
Epoch 27 / 100
eta=0.0 ==============================================================================================> 
867643ms 33580us/step - acc=0.662 loss=0.944 val_acc=0.633 val_loss=1.00 
Epoch 28 / 100
eta=0.0 ==============================================================================================> 
879659ms 34045us/step - acc=0.667 loss=0.921 val_acc=0.610 val_loss=1.05 
Epoch 29 / 100
eta=0.0 ==============================================================================================> 
865613ms 33502us/step - acc=0.675 loss=0.908 val_acc=0.628 val_loss=1.01 
Epoch 30 / 100
eta=0.0 ==============================================================================================> 
866699ms 33544us/step - acc=0.682 loss=0.893 val_acc=0.631 val_loss=0.998 
Epoch 31 / 100
eta=0.0 ==============================================================================================> 
881732ms 34125us/step - acc=0.687 loss=0.879 val_acc=0.642 val_loss=0.985 
Epoch 32 / 100
eta=0.0 ==============================================================================================> 
864608ms 33463us/step - acc=0.695 loss=0.855 val_acc=0.641 val_loss=0.986 
Epoch 33 / 100
eta=0.0 ==============================================================================================> 
824624ms 31915us/step - acc=0.701 loss=0.837 val_acc=0.633 val_loss=1.01 
Epoch 34 / 100
eta=0.0 ==============================================================================================> 
841613ms 32573us/step - acc=0.711 loss=0.822 val_acc=0.642 val_loss=1.02 
Epoch 35 / 100
eta=0.0 ==============================================================================================> 
829541ms 32105us/step - acc=0.718 loss=0.800 val_acc=0.640 val_loss=1.03 
Epoch 36 / 100
eta=0.0 ==============================================================================================> 
1029296ms 39837us/step - acc=0.722 loss=0.782 val_acc=0.651 val_loss=0.969 
Epoch 37 / 100
eta=0.0 ==============================================================================================> 
1141407ms 44176us/step - acc=0.721 loss=0.777 val_acc=0.634 val_loss=1.04 
Epoch 38 / 100
eta=0.0 ==============================================================================================> 
1141252ms 44170us/step - acc=0.729 loss=0.766 val_acc=0.638 val_loss=1.00 
Epoch 39 / 100
eta=0.0 ==============================================================================================> 
1146412ms 44369us/step - acc=0.735 loss=0.751 val_acc=0.649 val_loss=1.03 
Epoch 40 / 100
eta=0.0 ==============================================================================================> 
1140077ms 44124us/step - acc=0.735 loss=0.747 val_acc=0.648 val_loss=1.06 
Epoch 41 / 100
eta=0.0 ==============================================================================================> 
1003408ms 38835us/step - acc=0.751 loss=0.714 val_acc=0.630 val_loss=1.01 
Epoch 42 / 100
eta=0.0 ==============================================================================================> 
866414ms 33533us/step - acc=0.754 loss=0.708 val_acc=0.649 val_loss=1.05 
Epoch 43 / 100
eta=0.0 ==============================================================================================> 
865949ms 33515us/step - acc=0.759 loss=0.694 val_acc=0.619 val_loss=1.15 
Epoch 44 / 100
eta=0.0 ==============================================================================================> 
849025ms 32860us/step - acc=0.765 loss=0.685 val_acc=0.641 val_loss=1.04 
Epoch 45 / 100
eta=0.0 ==============================================================================================> 
818387ms 31674us/step - acc=0.766 loss=0.675 val_acc=0.650 val_loss=1.04 
Epoch 46 / 100
eta=0.0 ==============================================================================================> 
823833ms 31885us/step - acc=0.772 loss=0.661 val_acc=0.651 val_loss=1.04 
Epoch 45: early stopping.
Training finished in 44468.99 seconds.

Evaluating model on test data...
Test Loss: 1.1550
Test Accuracy: 0.6289

Saving model in browser-compatible format...
Model saved to ./face_emotion_model_browser

Disposing tensors and model...
TensorFlow.js backend memory usage: {
  unreliable: true,
  numTensors: 96972,
  numDataBuffers: 96972,
  numBytes: 345909120
}
